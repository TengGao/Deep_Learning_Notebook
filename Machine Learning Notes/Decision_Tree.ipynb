{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics to covers:\n",
    "\n",
    "- Decision Tree;\n",
    "- DBSCAN;\n",
    "- Time Series;\n",
    "- Anomaly detection;\n",
    "- Cross Validation;\n",
    "- Feature engineering.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n",
    "### 1. Spliting with Gini Index \n",
    "Gini index is used in the CART algorithm, with the following formula:\n",
    "$$ Gini = 1-\\sum^n_{i=1}p_i^2$$\n",
    "\n",
    "It has the properties:\n",
    "* Favor larger partitions;\n",
    "* Uses squared proportion of classes;\n",
    "* Perfectly classified, Gini scores would be zero;\n",
    "* Evenly distributed would be $1-(1/\\text{# classes})$;\n",
    "* Split on low Gini index;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gini(y):\n",
    "#     hist = np.bincount(y)\n",
    "#     N = np.sum(hist)\n",
    "#     return 1- sum([(i/N)**2 for i in hist])\n",
    "import numpy as np \n",
    "def gini(y):\n",
    "    hist = np.bincount(y)\n",
    "    N = np.sum(hist)\n",
    "    return 1-np.sum((hist/N)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Splitting with Information Gain and Entropy\n",
    "\n",
    "$$ Entropy = -\\sum^n_{i=1}p_i\\cdot log_2(p_i) $$\n",
    "\n",
    "* Favor splits with small counts but many unique values;\n",
    "* Weights probability of class by log(base=2) of the class probability;\n",
    "* A smaller value of the Entropy is better. That makes the difference between the parent node's entropy larger;\n",
    "* Information Gain is the Entropy of the parent node minus the entropy of the child nodes;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist/np.sum(hist)\n",
    "    return  -np.sum(ps * np.log2(ps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test out the difference of two loss function with a toy dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "Class = [\"A\",\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"B\"]\n",
    "var1 = [0,0,0,0,1,1,1,0,1,0]\n",
    "var2 = [33,54,56,42,50,55,31,-4,77,49]\n",
    "data = pd.DataFrame({'Class':Class, \"Var1\":var1, \"Var2\":var2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Gini Index is 0.48 and the Entropy is 0.9709505944546686\n"
     ]
    }
   ],
   "source": [
    "y = (data.Var1==1)*1\n",
    "gini_score = gini(y)\n",
    "entropy_score = entropy(y)\n",
    "print (f\"The Gini Index is {gini_score} and the Entropy is {entropy_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
